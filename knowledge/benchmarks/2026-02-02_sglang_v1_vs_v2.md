# sglang_v1 vs sglang_v2 — 2026-02-02

**GPU:** NVIDIA B200 (SM100)
**Definition:** `moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048`
**Tolerance:** rtol=0.2, atol=0.1, required-matched-ratio=0.85
**Config:** warmup=10, iterations=100, num_trials=5 (higher precision than default)

## Command

```bash
flashinfer-bench run --local "$FIB_DATASET_PATH" \
  --definitions moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048 \
  --solutions sglang_fp8_blockwise_moe_v1 sglang_fp8_blockwise_moe_v2 \
  --timeout 600 --rtol 0.2 --atol 0.1 --required-matched-ratio 0.85 \
  --warmup-runs 10 --num-iterations 100 --num-trials 5
```

## Differences

| Aspect | sglang_v1 | sglang_v2 |
|--------|-----------|-----------|
| SwiGLU | `torch.cat` swap + `silu_and_mul` kernel | `F.silu(c1[:, n:]) * c1[:, :n]` (zero-copy) |
| FP8 quant | Custom PyTorch `_per_token_group_quant_fp8` | `sgl_per_token_group_quant_fp8` native kernel |
| Workspace | 1GB (`1024*1024*1024` bytes) | 90KB (`90000` bytes) |

## Results (sorted by num_tokens)

All 19 workloads PASS for both.

| UUID | num_tokens | offset | V1 | V2 | V2/V1 |
|----------|--------:|-------:|-------:|-------:|------:|
| e05c6c03 | 1 | 32 | 13.80x | 23.42x | 1.70 |
| b8f4f012 | 7 | 192 | 21.78x | 23.02x | 1.06 |
| 8cba5890 | 14 | 0 | 22.75x | 27.55x | 1.21 |
| 2e69caee | 15 | 32 | 20.64x | 28.30x | 1.37 |
| a7c2bcfd | 16 | 224 | 18.86x | 22.36x | 1.19 |
| 6230e838 | 32 | 32 | 21.57x | 20.88x | 0.97 |
| f7d6ac7c | 52 | 160 | 19.61x | 22.24x | 1.13 |
| fc378037 | 53 | 32 | 23.08x | 26.90x | 1.17 |
| 76010cb4 | 54 | 128 | 23.70x | 25.00x | 1.06 |
| 81955b1e | 55 | 128 | 24.90x | 26.33x | 1.06 |
| 4822167c | 56 | 64 | 25.47x | 26.61x | 1.04 |
| 74d7ff04 | 57 | 96 | 24.47x | 27.41x | 1.12 |
| e626d3e6 | 58 | 64 | 24.27x | 24.04x | 0.99 |
| eedc63b2 | 59 | 160 | 24.28x | 25.54x | 1.05 |
| 5eadab1e | 62 | 96 | 23.79x | 27.28x | 1.15 |
| 8f1ff9f1 | 80 | 96 | 23.78x | 25.24x | 1.06 |
| 1a4c6ba1 | 901 | 96 | 17.80x | 20.44x | 1.15 |
| 58a34f27 | 11948 | 128 | 3.80x | 4.29x | 1.13 |
| 5e8dc11c | 14107 | 32 | 4.02x | 4.61x | 1.15 |

## Analysis

- **V1 average speedup:** 19.60x
- **V2 average speedup:** 22.71x
- **V2 is ~1.16x faster than V1 on average**

### By num_tokens regime
- **T=1:** V2 is 1.70x faster — the biggest win. At tiny batch sizes, 1GB workspace allocation in V1 dominates. V2's 90KB workspace eliminates this overhead.
- **Small batch (T=7-32):** V2 is 1.06-1.37x faster. Mixed — some workloads show clear wins, others are within noise.
- **Medium batch (T=50-80):** V2 is 1.04-1.17x faster. Consistent ~5-15% improvement.
- **Large batch (T=901):** V2 is 1.15x faster.
- **Very large batch (T >10K):** V2 is 1.13-1.15x faster. The ~13% gain is consistent, likely from native quant kernel replacing PyTorch quant.

### What helped most
1. **Workspace reduction (1GB -> 90KB):** Dominant factor for tiny batches (T=1 shows 1.70x). The 1GB `torch.empty` in V1 likely triggers a CUDA allocation on every call.
2. **Native FP8 quant kernel:** Consistent ~10-15% benefit across all batch sizes. `sgl_per_token_group_quant_fp8` is a fused CUDA kernel vs V1's multi-step PyTorch ops (reshape, abs, amax, clamp, div, cast).
3. **Zero-copy SwiGLU:** Eliminates the `torch.cat` copy of [m*topk, 4096] tensor. Benefit is modest since both approaches still materialize the intermediate tensor.
