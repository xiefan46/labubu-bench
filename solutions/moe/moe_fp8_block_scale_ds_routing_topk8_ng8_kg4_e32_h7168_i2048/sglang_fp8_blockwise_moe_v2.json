{
  "name": "sglang_fp8_blockwise_moe_v2",
  "definition": "moe_fp8_block_scale_ds_routing_topk8_ng8_kg4_e32_h7168_i2048",
  "description": "V2: Zero-copy PyTorch SwiGLU + sgl_kernel native FP8 quant + 90KB workspace.",
  "author": "sglang",
  "spec": {
    "language": "python",
    "target_hardware": [
      "NVIDIA B200"
    ],
    "dependencies": [
      "sgl_kernel"
    ],
    "entry_point": "main.py::run",
    "destination_passing_style": false
  },
  "sources": [
    {
      "path": "main.py",
      "content": "import torch\nimport torch.nn.functional as F\nfrom sgl_kernel import (\n    apply_shuffle_mul_sum,\n    fp8_blockwise_scaled_grouped_mm,\n    moe_fused_gate,\n    prepare_moe_input,\n    sgl_per_token_group_quant_fp8,\n    shuffle_rows,\n)\n\n# Fixed DeepSeek-V3/R1 geometry\nE_GLOBAL = 256\nE_LOCAL = 32\nTOP_K = 8\nN_GROUP = 8\nTOPK_GROUP = 4\nH = 7168       # hidden_size\nI = 2048       # intermediate_size\nBLOCK = 128\nFP8_MIN = -448.0\nFP8_MAX = 448.0\n\n\n@torch.no_grad()\ndef run(\n    routing_logits: torch.Tensor,\n    routing_bias: torch.Tensor,\n    hidden_states: torch.Tensor,\n    hidden_states_scale: torch.Tensor,\n    gemm1_weights: torch.Tensor,\n    gemm1_weights_scale: torch.Tensor,\n    gemm2_weights: torch.Tensor,\n    gemm2_weights_scale: torch.Tensor,\n    local_expert_offset: int,\n    routed_scaling_factor: float,\n):\n    device = hidden_states.device\n    T = hidden_states.shape[0]\n\n    # Convert scalar inputs\n    if isinstance(local_expert_offset, torch.Tensor):\n        local_expert_offset = int(local_expert_offset.item())\n    if isinstance(routed_scaling_factor, torch.Tensor):\n        routed_scaling_factor = float(routed_scaling_factor.item())\n\n    # --- Step 1: DeepSeek-V3 routing via moe_fused_gate ---\n    topk_weights, topk_ids = moe_fused_gate(\n        routing_logits.to(torch.float32).contiguous(),\n        routing_bias.to(torch.float32).contiguous(),\n        N_GROUP,\n        TOPK_GROUP,\n        TOP_K,\n        num_fused_shared_experts=0,\n        routed_scaling_factor=float(routed_scaling_factor),\n        apply_routed_scaling_factor_on_output=False,\n    )\n\n    topk_weights = topk_weights * routed_scaling_factor\n\n    # --- Step 2: Map global expert IDs to local [0, E_LOCAL) ---\n    local_mask = (topk_ids >= local_expert_offset) & (\n        topk_ids < local_expert_offset + E_LOCAL\n    )\n    local_ids = (topk_ids - local_expert_offset).clamp(0, E_LOCAL - 1)\n    topk_weights = topk_weights * local_mask.to(topk_weights.dtype)\n    topk_ids = local_ids.to(torch.int32)\n\n    m = T\n    topk = TOP_K\n    k = H\n    n = I\n    num_experts = E_LOCAL\n\n    # --- Step 3: Prepare MoE input ---\n    expert_offsets = torch.empty((num_experts + 1,), dtype=torch.int32, device=device)\n    problem_sizes1 = torch.empty((num_experts, 3), dtype=torch.int32, device=device)\n    problem_sizes2 = torch.empty((num_experts, 3), dtype=torch.int32, device=device)\n    a_map = torch.empty((m * topk,), dtype=torch.int32, device=device)\n    c_map = torch.empty((m * topk,), dtype=torch.int32, device=device)\n\n    prepare_moe_input(\n        topk_ids, expert_offsets, problem_sizes1, problem_sizes2,\n        a_map, c_map, num_experts, n, k,\n    )\n\n    # --- Step 4: Prepare activations ---\n    a_scale = hidden_states_scale.to(torch.float32).T.contiguous()\n    rep_a_q = shuffle_rows(hidden_states.contiguous(), a_map, (m * topk, k))\n    rep_a_scales = shuffle_rows(a_scale, a_map, (m * topk, k // BLOCK))\n\n    # --- Step 5: Transpose weights to SGLang convention ---\n    w1_q = gemm1_weights.transpose(1, 2)\n    w2_q = gemm2_weights.transpose(1, 2)\n    w1_scale = gemm1_weights_scale.to(torch.float32).transpose(1, 2)\n    w2_scale = gemm2_weights_scale.to(torch.float32).transpose(1, 2)\n\n    # --- Step 6: Allocate strides and scratch buffers ---\n    ab_strides1 = torch.full((num_experts,), k, device=device, dtype=torch.int64)\n    c_strides1 = torch.full((num_experts,), 2 * n, device=device, dtype=torch.int64)\n    ab_strides2 = torch.full((num_experts,), n, device=device, dtype=torch.int64)\n    c_strides2 = torch.full((num_experts,), k, device=device, dtype=torch.int64)\n\n    workspace = torch.empty(90000, device=device, dtype=torch.uint8)\n    a_ptrs = torch.empty((num_experts,), dtype=torch.int64, device=device)\n    b_ptrs = torch.empty((num_experts,), dtype=torch.int64, device=device)\n    out_ptrs = torch.empty((num_experts,), dtype=torch.int64, device=device)\n    a_scales_ptrs = torch.empty((num_experts,), dtype=torch.int64, device=device)\n    b_scales_ptrs = torch.empty((num_experts,), dtype=torch.int64, device=device)\n    a_sf_layout = torch.empty((num_experts, 5), dtype=torch.int32, device=device)\n    w_sf_layout = torch.empty((num_experts, 5), dtype=torch.int32, device=device)\n\n    # --- Step 7: GEMM1 ---\n    c1 = torch.empty((m * topk, 2 * n), device=device, dtype=torch.bfloat16)\n    fp8_blockwise_scaled_grouped_mm(\n        c1,\n        a_ptrs, b_ptrs, out_ptrs, a_scales_ptrs, b_scales_ptrs,\n        rep_a_q, w1_q, rep_a_scales, w1_scale,\n        ab_strides1, ab_strides1, c_strides1,\n        a_sf_layout, w_sf_layout,\n        problem_sizes1, expert_offsets[:-1],\n        workspace,\n    )\n\n    # --- Step 8: SwiGLU activation ---\n    intermediate = (F.silu(c1[:, n:]) * c1[:, :n]).to(torch.bfloat16)\n\n    # --- Step 9: Quantize intermediate to FP8 for GEMM2 ---\n    intermediate_q = torch.empty_like(intermediate, dtype=torch.float8_e4m3fn)\n    a2_scale = torch.empty(\n        (intermediate.shape[0], intermediate.shape[1] // BLOCK),\n        dtype=torch.float32, device=device,\n    )\n    sgl_per_token_group_quant_fp8(\n        intermediate, intermediate_q, a2_scale,\n        BLOCK, 1e-10, FP8_MIN, FP8_MAX, enable_v2=False,\n    )\n\n    # --- Step 10: GEMM2 ---\n    c2 = torch.empty((m * topk, k), device=device, dtype=torch.bfloat16)\n    fp8_blockwise_scaled_grouped_mm(\n        c2,\n        a_ptrs, b_ptrs, out_ptrs, a_scales_ptrs, b_scales_ptrs,\n        intermediate_q, w2_q, a2_scale, w2_scale,\n        ab_strides2, ab_strides2, c_strides2,\n        a_sf_layout, w_sf_layout,\n        problem_sizes2, expert_offsets[:-1],\n        workspace,\n    )\n\n    # --- Step 11: Weighted sum of expert outputs ---\n    output = torch.zeros((m, k), device=device, dtype=torch.bfloat16)\n    apply_shuffle_mul_sum(c2, output, c_map, topk_weights.to(torch.bfloat16))\n\n    return output\n"
    }
  ]
}